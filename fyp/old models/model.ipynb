{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSP650: PROJECT <br>\n",
    "# TITLE: GOOGLE PLAY APPLICATION CLASSIFICATION BASED ON SENTIMENT ANALYSIS OF REVIEWS <br>\n",
    "SUPERVISOR: MADAM UMMU FATIHAH BINTI MOHD BAHRIN <br>\n",
    "SUPERVISEE: AQIL KHAIRY BIN HAMSANI (2021856342) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from textblob import TextBlob\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "import joblib\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Reviews.csv\")\n",
    "data = data.sample(frac=0.05, random_state=42) #reduce samples (memory issues)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update nltk library (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords') #uncomment this to update stopwords package\n",
    "#nltk.download('vader_lexicon') #uncomment this to update lexicon package\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(nltk.corpus.stopwords.words('english'))\n",
    "data['original_content'] = data['content']\n",
    "data['content'] = data['content'].fillna('')\n",
    "data['content'] = data['content'].str.lower()\n",
    "data['content'] = data['content'].str.replace('[^\\w\\s]','', regex=True)\n",
    "data['content'] = data['content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ADDITIONAL PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace \"not\" and \"no\" with \"not_\" and \"no_\" respectively\n",
    "    text = re.sub(r'\\bnot\\b', 'not_', text)\n",
    "    text = re.sub(r'\\bno\\b', 'no_', text)\n",
    "\n",
    "    return text\n",
    "data['content'] = data['content'].apply(preprocess_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TextBlob to classify the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sentiment(text):\n",
    "#     blob = TextBlob(text)\n",
    "#     sentiment = blob.sentiment.polarity\n",
    "#     if sentiment >= 0.1:\n",
    "#         return 'positive'\n",
    "#     elif sentiment <= -0.1:\n",
    "#         return 'negative'\n",
    "#     else:\n",
    "#         return 'neutral'\n",
    "    \n",
    "# data['sentiment'] = data['content'].apply(lambda x: get_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "\n",
    "    if sentiment_scores['compound'] >= 0.1:\n",
    "        return 'positive'\n",
    "    elif sentiment_scores['compound'] <= -0.1:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "        \n",
    "data['sentiment'] = data['content'].apply(lambda x: get_sentiment(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some of the content and it's sentiment after using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: Good app\n",
      "Sentiment: positive\n",
      "\n",
      "Content: Excelent cbr reader, fast and smart, good reading tools, better app for for \n",
      "comics i found. Really no complains.\n",
      "Sentiment: positive\n",
      "\n",
      "Content: Amazing app\n",
      "Sentiment: positive\n",
      "\n",
      "Content: A friend recommended the app to me. I particularly like the clarity and the message function.\n",
      "Sentiment: positive\n",
      "\n",
      "Content: I couldn't drag the landmark to the desired point. It was frustrating. Pls \n",
      "fix it\n",
      "Sentiment: negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data = data.sample(n=5)\n",
    "for i, row in sample_data.iterrows():\n",
    "    print(f\"Content: {row['original_content']}\\nSentiment: {row['sentiment']}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split datasets for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['content'], data['sentiment'], test_size=0.1, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize text into numerical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text):\n",
    "    return [int(word in set(text.split())) for word in features]\n",
    "\n",
    "features = list(set(' '.join(X_train).split()))\n",
    "X_train_vect = np.array([vectorize_text(x) for x in X_train])\n",
    "X_test_vect = np.array([vectorize_text(x) for x in X_test])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save features as a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(features, 'features.joblib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameter grid to find the best max_depth for the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [None, 50, 100, 200]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Decision Tree model using the best max_depth found by grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth of the decision tree: 193\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "grid_search.fit(X_train_vect, y_train)\n",
    "clf = grid_search.best_estimator_\n",
    "\n",
    "tree_depth = clf.tree_.max_depth\n",
    "print(\"Depth of the decision tree:\", tree_depth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print each and the best depth for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: None, Mean Test Score: 0.8215743440233234\n",
      "Depth: 50, Mean Test Score: 0.8038872691933916\n",
      "Depth: 100, Mean Test Score: 0.8184645286686102\n",
      "Depth: 200, Mean Test Score: 0.8215743440233234\n",
      "Best Parameters:  {'max_depth': None}\n",
      "Best Score:  0.8215743440233234\n"
     ]
    }
   ],
   "source": [
    "for params in grid_search.cv_results_['params']:\n",
    "    depth = params['max_depth']\n",
    "    score = grid_search.cv_results_['mean_test_score'][grid_search.cv_results_['params'].index(params)]\n",
    "    print(f\"Depth: {depth}, Mean Test Score: {score}\")\n",
    "\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model using Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decision_tree_model.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the variable to file\n",
    "joblib.dump(clf, 'decision_tree_model.joblib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load back model into new variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = joblib.load('decision_tree_model.joblib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.43      0.48        75\n",
      "     neutral       0.66      0.76      0.71        88\n",
      "    positive       0.91      0.92      0.92       409\n",
      "\n",
      "    accuracy                           0.83       572\n",
      "   macro avg       0.71      0.70      0.70       572\n",
      "weighted avg       0.83      0.83      0.83       572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = dt_model.predict(X_test_vect)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select 5 random testing to display the result of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Review: gd appbut little bit dissatisfied adds\n",
      "Actual Sentiment: negative\n",
      "Predicted Sentiment: neutral\n",
      "\n",
      "Sample 2:\n",
      "Review: good story best\n",
      "Actual Sentiment: positive\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "Sample 3:\n",
      "Review: love unlock characters run races\n",
      "Actual Sentiment: positive\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "Sample 4:\n",
      "Review: really intersting apps whatever like tam tam\n",
      "Actual Sentiment: positive\n",
      "Predicted Sentiment: positive\n",
      "\n",
      "Sample 5:\n",
      "Review: love lt\n",
      "Actual Sentiment: positive\n",
      "Predicted Sentiment: positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_indices = np.random.choice(len(X_test), size=5, replace=False)\n",
    "\n",
    "random_X_test = X_test.iloc[random_indices]\n",
    "random_y_test = y_test.iloc[random_indices]\n",
    "\n",
    "# Predict sentiment for the selected samples\n",
    "random_X_test_vect = np.array([vectorize_text(x) for x in random_X_test])\n",
    "random_y_pred = dt_model.predict(random_X_test_vect)\n",
    "\n",
    "# Print the random samples and their predicted sentiment\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"Review: {random_X_test.iloc[i]}\")\n",
    "    print(f\"Actual Sentiment: {random_y_test.iloc[i]}\")\n",
    "    print(f\"Predicted Sentiment: {random_y_pred[i]}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single review test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: not bad\n",
      "Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "input_arr = [\"not bad\"]\n",
    "input_vect = np.array([vectorize_text(x) for x in input_arr])\n",
    "input_pred = dt_model.predict(input_vect)\n",
    "\n",
    "for i in range(len(input_pred)):\n",
    "    print(f\"Review: {input_arr[i]}\")\n",
    "    print(f\"Sentiment: {input_pred[i]}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1ae7672e6aebabab73014c2bd743854dd04fe7451e4e03180509a948c52199c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
